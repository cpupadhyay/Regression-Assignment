{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278897dd-7b85-437f-8e60-b6fb6054e794",
   "metadata": {},
   "source": [
    "# Theoritical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe5633-5c12-493a-b3b7-c6917d5a8728",
   "metadata": {},
   "source": [
    "1. What is Simple Linear Regression?\n",
    "   -> Simple Linear Regression is a statistical method used to model the relationship between two variables: one independent variable (X) and one                dependent variable (Y). It fits a straight line (Y = a + bX) to the data to predict Y based on X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70278f-714d-4489-9365-e79e182fe712",
   "metadata": {},
   "source": [
    "2. What are the key assumptions of Simple Linear Regression?\n",
    "   -> The key assumptions of Simple Linear Regression are:\n",
    "      -> Linearity: The relationship between X and Y is linear.\n",
    "      -> Independence: Observations are independent of each other.\n",
    "      -> Homoscedasticity: Constant variance of errors across all levels of X.\n",
    "      -> Normality: Errors (residuals) are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cc239-27a1-42b0-9f40-3fd782957f40",
   "metadata": {},
   "source": [
    "3. What does the coefficient m represent in the equation Y=mX+c?\n",
    "   -> The coefficient m in the equation Y = mX + c represents the slope of the line. It shows the change in Y for a one-unit change in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdf127-320a-4c10-9639-95f5d6cb6cc4",
   "metadata": {},
   "source": [
    "4. What does the intercept c represent in the equation Y=mX+c?\n",
    "   -> The intercept c in the equation Y = mX + c represents the value of Y when X is 0. It shows where the line crosses the Y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa8aa0-2f47-498d-8ab2-74000ce9ea60",
   "metadata": {},
   "source": [
    "5. How do we calculate the slope m in Simple Linear Regression?\n",
    "   -> The slope m is calculated using the formula:\n",
    "      m = Σ\\[(X - X̄)(Y - Ȳ)] / Σ\\[(X - X̄)²]\n",
    "      It measures how much Y changes with a one-unit change in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb7f6d-3659-4480-9cfc-270dde251995",
   "metadata": {},
   "source": [
    "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "   -> The purpose of the least squares method is to minimize the sum of the squared differences between the actual Y values and the predicted Y values.       It helps find the best-fitting line through the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d99a9-4a40-426b-a2d2-0ff61aefe552",
   "metadata": {},
   "source": [
    "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "   -> The coefficient of determination (R²) measures how well the regression line fits the data. It ranges from 0 to 1, where R² = 1 means perfect fit        and R² = 0 means the model explains none of the variation in Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd840160-839a-491c-a9a8-9a62d4d202df",
   "metadata": {},
   "source": [
    "8. What is Multiple Linear Regression?\n",
    "   -> Multiple Linear Regression is an extension of simple linear regression that models the relationship between one dependent variable (Y) and two or       more independent variables (X₁, X₂, ..., Xn). The equation is Y = b₀ + b₁X₁ + b₂X₂ + ... + bnXn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0769216-2ee9-44f9-ad15-a6f0faf291fa",
   "metadata": {},
   "source": [
    "9. What is the main difference between Simple and Multiple Linear Regression?\n",
    "   -> The main difference is that Simple Linear Regression has one independent variable, while Multiple Linear Regression involves two or more                independent variables to predict the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7a428-4662-4ccf-a1a0-54d9e3118593",
   "metadata": {},
   "source": [
    "10. What are the key assumptions of Multiple Linear Regression?\n",
    "    -> Key assumptions of Multiple Linear Regression are:\n",
    "       -> Linearity: Relationship between each independent variable and dependent variable is linear.\n",
    "       -> Independence: Observations are independent.\n",
    "       -> Homoscedasticity: Constant variance of errors across all levels of predictors.\n",
    "       -> Normality: Residuals are normally distributed.\n",
    "       -> No multicollinearity: Independent variables are not highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab10f81-64fe-4cbf-989e-0c1989e0f5be",
   "metadata": {},
   "source": [
    "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "    -> Heteroscedasticity means the variance of errors varies across levels of the independent variables instead of being constant. It can lead to             inefficient estimates and biased standard errors, making hypothesis tests and confidence intervals unreliable in a Multiple Linear Regression           model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d9bef-0878-4d3c-8955-f119775a6e88",
   "metadata": {},
   "source": [
    "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "    -> To improve a model with high multicollinearity, you can:\n",
    "    -> Remove or combine highly correlated variables.\n",
    "    -> Use Principal Component Analysis (PCA) to reduce dimensionality.\n",
    "    -> Apply regularization techniques like Ridge or Lasso regression.\n",
    "    -> Collect more data if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a986035-d438-408a-9bf4-dd5ecc023e0e",
   "metadata": {},
   "source": [
    "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
    "    -> Common techniques to transform categorical variables are:\n",
    "       -> One-Hot Encoding: Create binary columns for each category.\n",
    "       -> Label Encoding: Assign numeric labels to categories (best for ordinal data).\n",
    "       -> Target Encoding: Replace categories with the mean of the target variable.\n",
    "       -> Binary Encoding: Convert categories into binary digits (useful for high-cardinality features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76581c6-0ccb-4ac1-b711-3240b9c30575",
   "metadata": {},
   "source": [
    "14. What is the role of interaction terms in Multiple Linear Regression?\n",
    "    -> Interaction terms capture the effect of two or more variables combined, showing how the relationship between one independent variable and the           dependent variable changes depending on another variable. They help model more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a22ba1e-a304-49c3-b9f9-10933b3c2c21",
   "metadata": {},
   "source": [
    "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "    -> In Simple Linear Regression, the intercept is the predicted value of Y when the single X is zero.\n",
    "       In Multiple Linear Regression, the intercept represents the predicted Y when all independent variables are zero simultaneously, which may or may        not be meaningful depending on the data context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c0458f-e44b-4e37-ad24-6e9e6494a423",
   "metadata": {},
   "source": [
    "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "    -> The slope shows the rate of change in the dependent variable for a one-unit increase in an independent variable. It directly affects predictions        by determining how much the predicted value of Y changes as X changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0b612-aaaf-47bc-9acb-144238780928",
   "metadata": {},
   "source": [
    "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
    "    -> The intercept gives the baseline value of the dependent variable when all independent variables are zero, providing a starting point to                 understand the relationship and helping to anchor the regression line in the data’s context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95dcb8-c777-44e7-bd6f-187c63afb2a5",
   "metadata": {},
   "source": [
    "18. What are the limitations of using R² as a sole measure of model performance?\n",
    "    -> Limitations of using R² alone:\n",
    "       -> It doesn’t show if the model is biased or if predictors are significant.\n",
    "       -> It can increase with more variables, even if they’re not useful.\n",
    "       -> It doesn't indicate model accuracy on unseen data.\n",
    "       -> It doesn’t capture non-linear relationships or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b0e8a-1799-4000-a908-08ae311069f4",
   "metadata": {},
   "source": [
    "19. How would you interpret a large standard error for a regression coefficient?\n",
    "    -> A large standard error for a regression coefficient suggests that the estimate is unstable or imprecise. It indicates high variability in the           coefficient's value, making it harder to trust its statistical significance or predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970faaf9-7fa6-4244-a646-81bb78d55b8c",
   "metadata": {},
   "source": [
    "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "    -> Heteroscedasticity appears in residual plots as a funnel-shaped pattern—residuals spread out as fitted values increase. It's important to               address because it violates regression assumptions, leading to biased standard errors, which can make statistical tests unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb286c7a-2397-4b14-a20c-c3250a12b98f",
   "metadata": {},
   "source": [
    "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
    "    -> A high R² but low adjusted R² means the model includes irrelevant predictors that don't actually improve the model. Adjusted R² penalizes for           unnecessary variables, so the drop suggests overfitting or inclusion of non-informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1a744-77dc-42fb-b21a-a783a6d31d73",
   "metadata": {},
   "source": [
    "22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "    -> Scaling variables is important in Multiple Linear Regression because it:\n",
    "       -> Improves model stability when variables have different units or ranges.\n",
    "       -> Helps interpret coefficients more easily.\n",
    "       -> Is essential for regularized models like Ridge or Lasso to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14833c-79ae-4506-aee7-834e4d4d4098",
   "metadata": {},
   "source": [
    "23. What is polynomial regression?\n",
    "    -> Polynomial regression is a type of regression that models the relationship between the independent variable and the dependent variable as an            nth-degree polynomial. It captures non-linear patterns by adding higher-order terms like X², X³, etc., to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276031b8-e2ef-4ec0-9ae6-8ce94714a835",
   "metadata": {},
   "source": [
    "24. How does polynomial regression differ from linear regression?\n",
    "    -> Polynomial regression differs from linear regression by including non-linear terms (e.g., X², X³), allowing it to model curved relationships. In        contrast, linear regression only fits a straight line to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c2462-8d81-45dc-992b-eef4d57f555f",
   "metadata": {},
   "source": [
    "25. When is polynomial regression used?\n",
    "    -> Polynomial regression is used when the relationship between variables is non-linear, but you still want to use a regression-based approach. It's        helpful when data curves upward or downward, and a straight line doesn't fit well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24090e2e-7393-4040-abad-71b19843ef54",
   "metadata": {},
   "source": [
    "26. What is the general equation for polynomial regression?\n",
    "    -> The general equation for polynomial regression is:\n",
    "       Y = b₀ + b₁X + b₂X² + b₃X³ + ... + bnXⁿ + ε\n",
    "       where n is the degree of the polynomial and ε is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc667d5d-4fec-44bc-ad97-2e9d8d369f37",
   "metadata": {},
   "source": [
    "27. Can polynomial regression be applied to multiple variables?\n",
    "    -> Yes, polynomial regression can be applied to multiple variables by including polynomial terms and interactions for each variable. For example:\n",
    "       Y = b₀ + b₁X₁ + b₂X₂ + b₃X₁² + b₄X₂² + b₅X₁X₂ + ... + ε\n",
    "       This allows modeling complex, non-linear relationships in multivariable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ffea2a-5661-4f8c-900d-6b0b6655d871",
   "metadata": {},
   "source": [
    "28. What are the limitations of polynomial regression?\n",
    "    -> Limitations of polynomial regression:\n",
    "       -> Overfitting: High-degree polynomials can fit noise, reducing generalization.\n",
    "       -> Interpretability: Coefficients become hard to explain.\n",
    "       -> Extrapolation issues: Predictions outside the data range can be unreliable.\n",
    "       -> Computational complexity increases with degree and variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca58f8-1e81-48e9-bb4e-256d2f4c4ff9",
   "metadata": {},
   "source": [
    "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "    -> To evaluate model fit for selecting polynomial degree, use:\n",
    "       -> Cross-validation to test performance on unseen data.\n",
    "       -> Adjusted R² to penalize unnecessary complexity.\n",
    "       -> Root Mean Squared Error (RMSE) or Mean Absolute Error (MAE) for prediction accuracy.\n",
    "       -> Visual inspection of residual plots for patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9bf2c-3ee7-4598-9ba7-e10b3fc53ef0",
   "metadata": {},
   "source": [
    "30. Why is visualization important in polynomial regression? \n",
    "    -> Visualization is important in polynomial regression because it helps to:\n",
    "       -> See the shape of the relationship and whether the polynomial fits well.\n",
    "       ->Detect overfitting or underfittin by comparing curves to data points.\n",
    "       -> Identify outliers or patterns missed by numerical metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b6e99-fee3-49ed-8675-44a7316fca94",
   "metadata": {},
   "source": [
    "31. How is polynomial regression implemented in Python?\n",
    "    -> In Python, polynomial regression is implemented by:\n",
    "       -> Using `PolynomialFeatures` from scikit-learn to create polynomial terms.\n",
    "       -> Then applying a linear regression model on these transformed features.\n",
    "Example:\n",
    "python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degree = 3\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
